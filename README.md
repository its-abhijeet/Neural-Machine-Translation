# ğŸŒ Neural Machine Translation (English â†’ French)

This project implements a **Neural Machine Translation (NMT)** system using a **Sequence-to-Sequence (Seq2Seq) model with LSTMs** in TensorFlow/Keras.
It translates English sentences into French using the **fra-eng parallel dataset** and achieves **\~87% validation accuracy**.

---

## ğŸš€ Features

* Encoderâ€“Decoder **Seq2Seq architecture with LSTMs**
* Character-level tokenization + one-hot encoding
* Training with **categorical cross-entropy** loss
* Save & load trained `.h5` model for inference
* Unit-tested on simple translations (`"Go." â†’ "Va !"`, `"Run." â†’ "Fuyons !"`)
* Portable deployment (CPU/GPU)

---

## ğŸ“‚ Project Structure

```
ğŸ“¦ Neural-Machine-Translation
 â”£ ğŸ“œ data/               # fra-eng dataset (ManyThings.org)
 â”£ ğŸ“œ models/             # trained .h5 models
 â”£ ğŸ“œ src/
 â”ƒ â”£ preprocessing.py     # tokenization & encoding
 â”ƒ â”£ model.py             # Seq2Seq architecture
 â”ƒ â”£ train.py             # training loop
 â”ƒ â”£ inference.py         # prediction pipeline
 â”£ ğŸ“œ requirements.txt    # dependencies
 â”£ ğŸ“œ README.md           # project documentation
 â”— ğŸ“œ app.py (optional)   # Streamlit/Flask API (future enhancement)
```

---

## âš™ï¸ System Design

### ğŸ”¹ Architecture

1. **Input Layer** â†’ English sentence (one-hot encoded)
2. **Encoder LSTM** â†’ Generates context vector (`state_h, state_c`)
3. **Decoder LSTM** â†’ Predicts target words step-by-step
4. **Dense Layer (Softmax)** â†’ Probability distribution over target vocab
5. **Output Layer** â†’ Translated French sentence

---

## ğŸ“Š Dataset

* Source: [ManyThings.org (fra-eng)](http://www.manythings.org/anki/)
* Samples Used: **10,000 sentence pairs**
* Split: **80% training | 20% validation**

---

## ğŸ§‘â€ğŸ’» Installation & Setup

1. Clone the repo:

   ```bash
   git clone https://github.com/yourusername/neural-machine-translation.git
   cd neural-machine-translation
   ```
2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```
3. Download dataset (`fra.txt`) and place in `data/` folder.

---

## ğŸ‹ï¸ Training

```bash
python src/train.py
```

* Batch Size: `64`
* Epochs: `100`
* Latent Dim: `256`

The trained model is saved as:

```
models/nmt_model.h5
```

---

## ğŸ”® Inference / Translation

Run the inference script:

```bash
python src/inference.py --sentence "How are you?"
```

Example output:

```
English: How are you?
French: Comment allez-vous ?
```

---

## ğŸ“ˆ Results

* **Validation Accuracy**: \~87%
* **Strengths**: Simple sentences translated accurately
* **Limitations**: Long/complex sentences may show errors due to LSTM bottleneck

---

## ğŸ”® Future Enhancements

* Replace one-hot encoding with **word embeddings (Word2Vec, GloVe, FastText)**
* Upgrade to **Transformer-based models (BERT, mBART, T5)**
* Add **attention mechanism** for better alignment
* Deploy via **Flask/Streamlit API**
* Multi-language translation support

---

## ğŸ“š References

* Dataset: [ManyThings.org Englishâ€“French Corpus](http://www.manythings.org/anki/)
* Seq2Seq: [Sutskever et al., 2014](https://arxiv.org/abs/1409.3215)
* TensorFlow Documentation: [Seq2Seq Models](https://www.tensorflow.org/tutorials/text/nmt_with_attention)

---

## ğŸ‘¨â€ğŸ’» Author

**Abhijeet**
ğŸ”— [GitHub Repository](https://github.com/yourusername/neural-machine-translation)

âš¡ Now, do you want me to also **generate a `requirements.txt` file** (with exact versions for TensorFlow, Keras, NumPy, etc.) so your project is instantly reproducible?
